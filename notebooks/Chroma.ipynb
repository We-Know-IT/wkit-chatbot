{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting data to documents\n",
    "This notebook is for using the data we obtained in the dataframe and representing it in a langchain Document object. This is a data structure very applicable to Data Science and NLP tasks, since it allows us to separate the actual text content we want to use for our embeddings from the metadata tags, such as source and url. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Importing essential packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "import sys\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Loading the df from memory\n",
    "\n",
    "Since we already extracted the data in the scraping notebook, we can just load it from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - The langchain Document\n",
    "\n",
    "Now, the idea behind this data structure is that it is very suitable for NLP tasks. It has one attribute, page_content, that is the plain text we intend to embed and store in a vectorized form. Additionally, it has a metadata field which is a customizable dictionary where we can store whatever we want. This content is not touched by the embedding model, so we can use this downstream for filtering etc.\n",
    "\n",
    "This function simply loops through the dataframe and builds a Document instance. This is where you would typically introduce as much metadata as you can find, since it is cheap in terms of storage and gives you much more dynamic possibilities in future alterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df_to_langchain_documents(df):\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame into a list of LangChain Document objects.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to convert.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of LangChain Document objects.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for _, row in df.iterrows():\n",
    "        doc = Document(\n",
    "            page_content=row['text'],\n",
    "            metadata={\n",
    "                'key': row['key'],\n",
    "                'url': row['url'],\n",
    "                'category': row['category']\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df_to_langchain_documents(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Nyfiken På We Know IT? | Om Oss Våra kunder Våra tjänster Konsultuthyrning Webbutveckling Apputveckling UX/UI - Design Digital marknadsföring Hosting & förvaltning Om oss Karriär Kontakta oss Våra kunder Våra tjänster Konsultuthyrning Webbutveckling Apputveckling UX/UI - Design Digital Marknadsföring Hosting & Förvaltning Om oss Karriar Kontakta oss Varför We Know IT? Vi är IT-konsultbolaget som satsar på studerande och nyexade talanger med höga ambitioner och drivkrafter.\\xa0Experter på utveckling, design och digital strategi, We Know IT helt enkelt. \\u200bMål, vision & sådant gött Vårt mål är att vara det självklara valet för studenter att starta sin karriär på, och det självklara valet för företag som behöver motiverad och innovativ kompetens. Det är inte alla som får chansen att jobba med morgondagens skarpaste konsulter. Vi får det varje dag, och du som kund eller samarbetspartner får stora möjligheter genom att välja oss. Vi vill ha kul på vägen och leverera över förväntan så ofta som möjligt. Det gör vi genom att vara lyhörda, ha öppen kommunikation och starka relationer. Vi ger våra WKITare frihet och tror på varje persons förmåga att lösa utmaningar på bästa sätt. Dessutom är vi helt digitala, vilket möjliggör för oss att knyta kontakter runt om i hela Sverige. Vi värdesätter att ge frihet åt våra konsulter och att hjälpa dem uppnå sina mål i början på karriären. anställda 60+ Levererade projekt/år 100+ tech 100% Har du en idé? Berätta mer! Tjänster Menu Konsultuthyrning Webbutveckling Apputveckling UX/UI - Design Digital Marknadsföring Hosting & Förvaltning close Se mer Menu Kunder Om oss Karriär Kontakt Policy & Cookies close Kontakt hello@weknowit.se +46 (0) 8–194 811 Besök oss Kungsgatan 64 111 22 Stockholm We Know IT är techbolaget som sedan 2008 levererat innovativa plattformar och uppdrag inom utveckling, design och digital marknadsföring. Vi strävar efter långsiktiga samarbeten, leveranser över förväntan och ett agilt arbetssätt och kommunikation. © We Know IT Sweden AB close arrow-circle-o-down bars linkedin angle-down ellipsis-v instagram' metadata={'key': 'om-oss', 'url': 'https://www.weknowit.se/om-oss/', 'category': 'general'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n",
    "chunked_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_key = None\n",
    "counter = 0\n",
    "\n",
    "for chunk in chunked_docs:\n",
    "    key = chunk.metadata.get(\"key\")\n",
    "    if key != current_key:\n",
    "        current_key = key\n",
    "        counter = 0\n",
    "    chunk.metadata[\"chunk_id\"] = f\"{key}_{counter}\"\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - The embedding functions\n",
    "These are supporting functions, based on the already existing functions and methods on HuggingFace. It can be nice to write these manually in order to have more control of what we do with the text chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "def doc_embedding(embedding_model: str, \n",
    "                  model_kwargs: dict={'device':'cpu'}, \n",
    "                  encode_kwargs: dict={'normalize_embeddings':True},\n",
    "                  cache_folder: Optional[str]=None,\n",
    "                  multi_process: bool=False\n",
    "                  ) -> HuggingFaceEmbeddings:\n",
    "    embedder = HuggingFaceEmbeddings(\n",
    "        model_name = embedding_model,\n",
    "        model_kwargs = model_kwargs,\n",
    "        encode_kwargs = encode_kwargs,\n",
    "        cache_folder = cache_folder,\n",
    "        multi_process = multi_process\n",
    "    )\n",
    "    return embedder\n",
    "\n",
    "def get_API_embedding(text, model):\n",
    "    embedder = doc_embedding(model)\n",
    "    embedding = embedder.embed_query(text)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Create the vector databases\n",
    "\n",
    "In order to do this, we need to download an embedding model. In this example, we download the model locally, since it is rather small. The task is then to use our created langchain Documents and the embedding models to locally persist a directory, which is the Chroma collection and the Chroma DB we will then use for context retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axelandersson/Desktop/github/wkit-chatbot/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/axelandersson/Desktop/github/wkit-chatbot/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#model = \"intfloat/multilingual-e5-large-instruct\"\n",
    "model = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "embedding_model = doc_embedding(model)\n",
    "\n",
    "persist_directory = \"e5_ml_db\"\n",
    "\n",
    "# vectordb = Chroma.from_documents(documents=chunked_docs, \n",
    "#                                  embedding=embedding_model, \n",
    "#                                  persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(embedding_function=embedding_model, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Webbapplikationer som byggts genom åren\"\n",
    "context = vectordb.similarity_search_with_relevance_scores(query, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Retrieve the value of the environment variable\n",
    "openai_key = os.environ.get(\"OPENAI_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axelandersson/Desktop/github/wkit-chatbot/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=os.environ.get(\"OPENAI_KEY\"),\n",
    "    model='gpt-3.5-turbo-1106',\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (SystemMessage, HumanMessage, AIMessage)\n",
    "\n",
    "system_prompt = \"\"\"Du är en hjälpsam AI assistent, specialiserad på att svara på frågor om ett IT-konsultbolag som\n",
    "                heter We Know IT. Du kommer att få frågor samt utvald information, vilken du kan använda\n",
    "                för att svara på frågan. Svara på svenska.\"\"\"\n",
    "\n",
    "def get_prompt(query: str, vectordb):\n",
    "    # Retrieve 10 chunks with relevance scores\n",
    "    context_results = vectordb.similarity_search_with_relevance_scores(query, 10)\n",
    "    \n",
    "    # Extract the page_content from each context document\n",
    "    context = \"\\n\".join([doc.page_content for doc, score in context_results])\n",
    "    \n",
    "    # Construct the final prompt\n",
    "    user_prompt = f\"\"\"Svara på följande fråga: {query}.\n",
    "\n",
    "Du kan använda följande information för att generera ditt svar:\n",
    "{context}\"\"\"\n",
    "\n",
    "    return user_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_prompt(\"Hur bygger We Know It sina webbapplikationer?\", vectordb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We Know IT bygger sina webbapplikationer genom en effektiv utvecklingsfas där de använder sitt unika ramverk för att säkerställa högsta kvalitet och kontinuerlig rapportering. De lägger stor vikt vid att SEO-anpassa allt innehåll i termer av sökordsdensitet, ordmängd och kvalitet. När webbutvecklingen är färdig och det nya innehållet har integrerats, genomförs kvalitetssäkring och lansering av webbplatsen. We Know IT fokuserar också på konverteringsoptimering för att förbättra användarupplevelsen och öka konverteringar på webbplatsen.\n"
     ]
    }
   ],
   "source": [
    "query = \"Hur bygger We Know It sina webbapplikationer?\"\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=get_prompt(query, vectordb))\n",
    "]\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
